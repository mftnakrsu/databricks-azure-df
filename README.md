
# Azure Data Platform Project: End-to-End Data Pipeline with Unity Catalog

This repository demonstrates a modular, production-ready data platform built on **Azure** and **Databricks**, featuring an end-to-end data pipeline architecture with **Unity Catalog** for secure governance.

##  Project Structure

```

üìÅ set-up/                    ‚Üí Initial resource configuration & environment setup  
üìÅ ingestion/                 ‚Üí Data ingestion scripts (e.g. Azure Data Factory, Databricks AutoLoader)  
üìÅ raw/                       ‚Üí Raw data zone definitions (Bronze layer)  
üìÅ trans/                     ‚Üí Transformation logic (Silver/Gold layer ETL scripts)  
üìÅ analysis/                  ‚Üí Analytical workflows, aggregations & KPI dashboards  
üìÅ demo/                      ‚Üí Demo datasets or notebook-based walkthroughs  
üìÅ includes/                  ‚Üí Shared config, parameters, constants  
üìÅ utils/                     ‚Üí Utility functions and helper modules  
üìÅ unity-catalog-introduction/ ‚Üí Basics of Unity Catalog & governance setup   
üìÅ unity-catalog-capabilities/ ‚Üí Access control, lineage, schema enforcement examples  
üìÅ unity-catalog-mini-project/ ‚Üí A mini project using Unity Catalog on a small dataset  

```

##  Technologies Used

- **Azure Data Factory** (ADF) ‚Äì Batch ingestion from SQL sources  
- **Azure Data Lake Gen2** ‚Äì Staging, raw, and curated storage zones  
- **Databricks** ‚Äì Notebooks, AutoLoader, Delta Lake, PySpark transformations  
- **Unity Catalog** ‚Äì Centralized governance, fine-grained access control  
- **Python** ‚Äì Transformation and utility scripts  
- **SQL** ‚Äì Analytical queries & transformations

##  Key Features

- Modular folder-based pipeline
- Multi-layer architecture (Bronze ‚Üí Silver ‚Üí Gold)
- Unity Catalog security policies and usage examples
- Reusable utilities for data quality and monitoring
- Demo notebooks and mini-projects for hands-on practice

##  Who is this for?

This repository is ideal for:

- Data engineers working with Azure and Databricks
- Learners exploring modern data lakehouse architectures
- Teams implementing Unity Catalog for secure governance
- Professionals preparing for data engineer interviews

##  Getting Started

1. Clone the repository  
2. Follow `set-up/README.md` to configure your Azure + Databricks environment  
3. Start from `unity-catalog-introduction/` if you‚Äôre new to Unity Catalog  
4. Explore `ingestion/` and `trans/` for pipeline implementation  
5. Check `analysis/` for visualization or reporting logic

---

##  Future Work

- Stream ingestion with Event Hub
- CI/CD with GitHub Actions for Databricks Notebooks
- Metadata lineage with Unity Catalog APIs

---

##  License

MIT License. See `LICENSE` for details.

---

> Built with ‚ù§Ô∏è to explore enterprise-grade data solutions on Azure + Databricks.




